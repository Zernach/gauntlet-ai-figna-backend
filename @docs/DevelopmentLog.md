# ü§ñ Development Log

I definitely made mistakes along the way that slowed me down by many hours and forced me to restart from scratch twice (once on front-end and once on back-end) like pulling off band-aids, but these were learning opportunities. My most time-consuming mistake was trying to shoe-horn this project into the infrastructure of my other portfolio projects, but because my portfolio is such a large codebase, it caused sloppy AI behavior and a cluttered mess, which would've been an awful submission. There must certainly be better strategies for working in large codebases, but that is not this week. 

My strategy for developing this project was very incremental:
- started with the documentation (PRD, task list, architecture diagram)
- prompt my way to generating the back-end server and SQL 
- deploy the back-end server to Railway (websocket and http)
- launch a Supabase for authentication and run the SQL to create tables
- begin frontend: start with blank full-screen canvas & authentication
- initiate websocket connection upon successful login
- enable multiplayer mode & adding circles/rectangles to the canvas
- and the rest of the details were all downhill from there!

### ‚öôÔ∏è Prompting Strategies
- I'm astounded at how much faster Cursor is compared to Codex CLI
- I have found it helpful to refer to variables by their names instead of synonyms
- In 99% of the the cases, I used the "Agent" feature in Cursor for generating code
- But, I also enjoyed using the "Ask" feature to learn some information, and then following up with an "Agent" prompt
- In the beginning, when there were many crashing bugs, it was nice to copy/paste errors and let Cursor figure it out
- Although my two projects are in two different repositories, I let Cursor run in a workspace with both backend & frontend repos

###‚öôÔ∏è Code Analysis (rough percentages)
- **Backend src**: ~99% AI-generated (manual changes were mostly just deletions)
- **Frontend src**: ~99% AI-generated (manual changes were mostly just deletions)
- **Config/SQL/docs**: ~99% AI-generated (manual changes were mostly just deletions)
- **Overall**: Approximately 50‚Äì60% AI-generated across this phase. Estimation based on authorship of diffs, commit descriptions, and characteristic patterns in generated code.

### ‚öôÔ∏è Tools & Workflow
- **Primary tools**: Cursor (GPT-5), TypeScript compiler, local linter/formatter, Supabase (DB/Auth/Realtime), Vite (frontend), Node/Express (backend).
- **AI integration**: Used Cursor inline edits and multi-file reasoning to scaffold routes, middleware, hooks, and utility modules. Leveraged semantic search to navigate code, and constrained edits to minimal diffs for safety.
- **Backend workflow**: Iterated on `Express` routes, middleware (`auth`, `rateLimiter`, `errorHandler`), `services` (canvas/presence/user), and WebSocket server. Validated types and runtime behavior after each edit.
- **Frontend workflow**: Built/iterated React components (`Canvas`, `CanvasCursor`, `CanvasShape`, `AuthModal`, `AuthButton`), hooks (`useWebSocket`), and lib (`api`, `websocket`, `supabase`). Verified live behavior with Vite dev server.
- **Docs & guardrails**: Captured decisions in `@docs`, avoided production-impacting commands without explicit approval, and kept changes incremental.

### ‚öôÔ∏è Strengths & Limitations
- **Where AI excelled**
  - Rapid scaffolding of boilerplate (routes, middleware, hooks) and consistent patterns across files.
  - Enumerating edge cases and adding defensive checks in middleware and services.
  - Translating high-level requirements into clear, typed interfaces and utilities.
  - Supabase integration snippets and auth flows, with sensible defaults.
  - Better code using context from both the front-end and back-end
- **Where AI struggled**
  - Multi-file refactors that require deep project-wide invariants (e.g., presence broadcasts vs. canvas mutation ordering).
  - Subtle WebSocket timing/race issues and reconnection heuristics without live telemetry.
  - Domain-specific UX polish and canvas interaction nuances that need hands-on testing.
  - Build/config changes that depend on local environment and secrets. I want to learn how to allow Cursor to edit my `.env.sample` file.

### ‚öôÔ∏è Key Learnings (working with AI coding agents)
- **Small, verifiable steps win**: Constrain edits to one file/function; immediately re-run type checks and manual tests.
- **Be explicit about constraints**: Performance targets, API stability, and error handling expectations should be stated up front.
- **Prefer patterns over snippets**: Ask for a reusable pattern (e.g., retry/backoff strategy) and then apply it consistently.
- **Keep humans in the loop**: Use AI to draft; humans finalize domain logic, UX, and environment-sensitive code.
- **Document assumptions**: Capture non-obvious decisions (auth boundaries, realtime semantics) in `@docs` to prevent drift.
